#+TITLE: Shell Field Guide
#+AUTHOR: Raimon Grau
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:nil arch:headline
#+OPTIONS: author:t c:nil creator:comment d:(not "LOGBOOK") date:nil
#+OPTIONS: e:t email:nil f:t inline:t p:nil pri:nil stat:t
#+OPTIONS: tags:t tasks:t tex:t timestamp:t todo:t |:t
#+EXCLUDE_TAGS: noexport
#+KEYWORDS: bash zsh shell
#+LANGUAGE: en
#+SELECT_TAGS: export

#+SETUPFILE: https://fniessen.github.io/org-html-themes/setup/theme-readtheorg.setup

#+OPTIONS: reveal_center:nil timestamp:nil
#+REVEAL_THEME: black

# toc:nil num:nil

* Introduction
  This booklet is intended to be a catalog of tricks and techniques
  you should be using if you're doing any sort of scripting. I'll try
  to keep the rethoric to the very minimal because that way you can
  enjoy the content in less time.

  You probably have some amount of sh/bash/zsh in your stack that
  probably started as 1 off scripts, and probably later on started
  growing and being copypasted everywhere in your pipelines, or your
  coworkers use for their own use (with some variations), etc. Those
  scripts are very difficult to kill and they have a very high
  mutation rate.
* shell
  No matter if you use Linux, Mac, or Win, you should be living most
  of the time in a shell to enjoy the content shown here. Some value
  comes from the automated scripts, and some comes from the daily
  usage and refinement of your helper functions, aliases, etc.

  In general the examples here are ment to run in Bash or Zsh, which
  are compatible for the most part.
* Level
  These examples are based on non-trivial real world code I've written
  that I haven't seen applied in many places over the net. Some of the
  snippets are stolen from public repos.
* bash
** Use Shellcheck
   First, let's get that out of the way. This is low hanging
   fruit. And you will get the most of this booklet by following it.

   A lot of the most common errors we usually make are well known
   ones. And in fact, we all usually fail in similar ways. Bash is
   known for being error prone when dealing with testing variable
   values, string operations, or flaky subshells and pipes.

   Installing [[https://www.shellcheck.net/][shellcheck]] will flag you many of those ticking bombs.

   No matter which editor you are using, but you should be able to
   install a plugin to do automatic checks while you're editing.

   In emacs' case, the package is called [[https://github.com/federicotdn/flymake-shellcheck][flymake-shellcheck]], and a
   quick configuration for it is:

   #+begin_src elisp
   (use-package flymake-shellcheck
     :ensure t
     :commands flymake-shellcheck-load
     :init
     (add-hook 'sh-mode-hook 'flymake-shellcheck-load))
   #+end_src

   Shellcheck is available on most distros, so it's just an =apt=,
   =brew=, or =nix-env= away.

** use [[
   Unless you want your script to be POSIX compliant, use =[[= instead
   of =[=. =[= is a regular command. It's like =ls=, or =true=. You can
   check it by searching for a file named =[= in your path.

   Being a normal command it always evaluates its params, like a
   regular function. On the other hand though, =[[= is a special bash
   operator, and it evaluates the parameters lazily.

   #+begin_src bash

   # [[ does lazy evaluation:
   [[ a = b && $(echo foo >&2) ]]

   # [ does not:
   [ a = b -a "$(echo foo >&2)â€ ]
   #+end_src
   Ref: https://lists.gnu.org/archive/html/help-bash/2014-06/msg00013.html

** eval?
   When you have mostly small functions that are mostly pure, you
   compose them like you'd do in any other language.

   In the following snippet, we are in a release script. Some step
   builds a package inside an image, another step tests a package
   already built.

   A nice way to build ubuntus, for example, is to add an ARG to the
   Dockerfile so we can build several ubuntu versions using the same
   file.

   It'd look something like this:
   #+begin_src Dockerfile
   ARG VERSION
   FROM ubuntu:$VERSION

   RUN apt-get ...
   ...
   #+end_src

   We build that image and do all the building inside it, mounting a
   volume shared with our host, so we can extract our =.deb= file
   easily.

   After that, you probably want to do some smoke tests on the
   package, so the idea would be to install the =.deb= file in a fresh
   ubuntu image.

   Let's pick the same base image we picked to build the package.
   #+begin_src bash
   # evaluate the string "centos:$VERSION" (that comes from
   # centos/Dockerfile) in the current scope
   local VERSION=$(get_version $DISTRO)
   run_tests "$PACKAGE_PATH" "$(eval echo $(awk '/^FROM /{print $2; exit}' $LOCAL_PATH/$(get_dockerfile_for $DISTRO)))"
   #+end_src

   The usage of eval is there to interpolate the string that we get
   from the =FROM= in the current environment.

   WARNING: You know, anything that uses =eval= is dangerous per
   se. Do not use it unless you know very well what you're doing AND
   the input is 100% under your control. Usually, more restricted
   commands can achieve what you want to do. In this particular case,
   you could use =envsubst=, or just manually replace =$\{?VERSION\}?=
   in a sed.

   #+begin_src bash
    test_release "$PACKAGE_PATH" $(awk '/^FROM /{print $2; exit}' $LOCAL_PATH/$(get_dockerfile_for $DISTRO) | sed -e "s/\$VERSION/$VERSION/")
   #+end_src

* Basics

** Overview
   In this section, we're covering the parts of the basics that are
   not so basic after all, or that are more unique in shellscripting
   languages.

** Booleans and Conditionals
   In any shell, =foo && bar= will execute =bar= only if =foo=
   succeeded. That means that =foo= returned 0. That means that to &&
   (which you read like "and"), 0 is true. so yes. 0 is true, and
   other values are false.

** Functions
   Functions are functions. They receive arguments, and they return a
   value.

   The special thing about functions in shell is that they also can use
   the file descriptors of the process. That means that they "inherit"
   STDIN, STDOUT, STDERR (and maybe more).

   Use them.

   Another point is that function names can be passed as parameters,
   because they are passed as strings, but you can call them inside as
   functions again.

   #+begin_src bash
   f() {
     $1 hi
   }

   f echo
   f touch # will create a file 'hi'
   #+end_src

   #+RESULTS:
   : hi

** Variables
   By default variables are global, to a file. No matter if you assign
   them for the first time inside a function, or at the top leve.
   #+begin_src bash
   foo=3
   bar=$foo
   f() {
     echo $foo
   }
   f
   #+end_src

   #+RESULTS:
   : 3

   #+begin_src bash
   f() {
     bar=1
   }
   f
   echo $bar
   #+end_src

   #+RESULTS:
   : 1

   You make a variable local to a function with =local=. Use it as
   much as you can (kinda obvious).
   #+begin_src bash
   myfun() {
     local bar
     bar=3
     echo $bar
   }

   bar=4
   echo $bar
   myfun
   echo $bar
   #+end_src

   #+RESULTS:
   | 4 |
   | 3 |
   | 4 |
** Interpolation
   We previously saw that functions can be passed around as strings,
   and be called later on.

   Something that might not be obvious is that the string can be
   created from shorter strings, and that allows for an extra
   flexibility, that comes with its own dangers, but it's a very
   useful pattern to dispatch functions based on user input or
   function outputs.
   #+begin_src bash
   l=l
   s=s
   $l$s .
   #+end_src

   #+RESULTS:
   | book.html  |
   | book.html~ |
   | book.org   |
   | readme.org |

** dispatch functions using args
   A nice usage of the previous technique is using user's input as a
   dispatching method.

   You've probably seen this pattern already:

   #+begin_src bash
   while [[ $# -gt 0 ]]; do

   case $1 in
     foo)
       foo
       ;;
     *)
       exit 1
       ;;
   esac
   shift
   done
   #+end_src

   And it is useful for its own good, and flexible.

   But for some simpler cases, we can dispatch based on the variable
   itself:

   #+begin_src bash
   cmd_foo() {
    do-something
   }

   cmd_$1
   #+end_src

   The problem with this is that in case we supply a =$1= that doesn't
   map to any =cmd_$1= we'll get something like

   #+begin_src bash
   bash: cmd_notexisting: command not found
   #+end_src

** command_not_found_handle
   Here's a detail on a kinda obscure bash (only bash) feature.

   You can set a hook that will be called when bash tries to run a
   command and it doesn't find it.

   #+begin_src bash
   command_not_found_handle() {
     echo "$1 is not a correct command. Cmds allowed:"
     echo "$(typeset -F | grep cmd_ | sed -e 's/.*cmd_/cmd_/')"
   }

   cmd_foo() {
     echo "foo"
   }

   cmd_baz() {
     echo "baz"
   }
   cmd_bar
   #+end_src

   you can unset the function =command_not_found_handle= to go back to
   the normal behavior.


** Return Values for Conditionals
   =if= 's conditions can be bound to the return values of
   commands. That's a known thing, but lots of code you see around
   rely on =[[]]== to test the return values of commands/functions
   anyway.

   #+begin_src bash
   if echo "foo" | grep "bar" ; then
     echo "found!"
   fi
   #+end_src

   This is much clearer than
   #+begin_src bash
   if [[ ! -z $( echo "foo" | grep "bar") ]]; then
     echo "found!"
   fi
   #+end_src

   As easy and trivial as it seems, this way of thinking pushes you
   forward to thinking on creating smaller functions that check the
   conditions and =return= 0 or non 0. It's syntactically smaller, and
   usually makes you play by the rules of the commands, more than just
   finding your way around the output strings.

   #+begin_src bash
   if less_than $package "1.3.2"; then
     die "can't proceed"
   fi
   #+end_src

** Do work on loop conditions
   Although not commonly seen used (and there might be a reason for
   it, who knows), =while= conditions are just plain commands, so you
   can put other stuff than =[]= tests there.

   Heres's an idiomatic way to iterate through all the arguments of a
   function while consuming the =$*= array.

   #+begin_src bash
   while(($#)) ; do
     #...
     shift
   done
   #+end_src

   And here's a pseudo-repl that keeps shooting one-off commands.

   #+begin_src bash
   while rlwrap -o -S'>> ' kong runner; do :; done
   #+end_src

   Note: =:= is a nop builtin in bash.
** One Branch Conditionals
   The usual conditionals one sees everywhere look like =if=.
   #+begin_src bash
   if [[ some-condition ]]; then
     echo "yes"
   fi
   #+end_src

   This is all good and fine, but in the same vein of using the least
   powerful construct for each task, it's nice to think of the one way
   conditionals in the form of =&&= and =||= as a way to explicitly
   say that we don't want to do anything else when the condition is
   not met. It's a hint to the reader.

   #+begin_src bash
   some-condition || {
      echo "log: warning!"
   }

   other-condition && {
      echo "log: all cool"
   }
   #+end_src

   This conveys the desire of doing something *just* in one case, and
   that the negation of this is not interesting at all.

   There are lots of references to this, but I like this recent post
   where it explains it for arrays in higher level languages like ruby:
   https://jesseduffield.com/array-functions-and-the-rule-of-least-power/

* interactive
** Save your small scripts
   Rome wasn't built in a day, and like having a journal log, most of
   the little scripts you create, once you have enough discipline will
   be useful for some other cases, and your functions will be
   reusable.

   Save your scripts into files early on, instead of crunching
   everything in the repl. learn how to use a decent editor that
   shortens the feedback cycle as much as possible.

** Increased Interactivity
   Knowing your shell's shortcuts for interactive use is a must. The
   same way you learned to touchtype and you learned your editor, you
   should learn all the shortcuts for your shell. Here's some simple
   ones.

   | key    | action                        |
   |--------+-------------------------------|
   | ctrl-r | reverse-history-search        |
   | c-a    | beginning-of-line             |
   | c-e    | end-of-line                   |
   | c-w    | delete-word-backwards         |
   | c-k    | kill-line (from point to eol) |
   | c-p    | previous-line                 |
   | c-n    | next-line                     |
   | a-.    | insert last agument           |
   | a-/    | dabbrev-expand                |

** Aliases
   Aliases are very simple substitutions of commands for a sequence of
   other commands.  Usual example is

   #+begin_src bash
   alias ls='ls --auto-color'
   #+end_src

   Now let's move on to the interesting stuff.

** functions can generate aliases

   Aliases live in a global namespace for the shell, so no matter
   where you define them, they take effect globally, possibly
   overwriting older aliases with the same name.

   Well, it's not lexical scope (far from it), but using aliases you
   can create a string that snapshots the value you want, and capture
   it to run it later.

   Some fun stuff:

   - aliasgen. Create an alias for each directory in
     ~/workspace/. This is superceeded by =CDPATH=, but the trick is
     still cool.
   #+begin_src zsh
   aliasgen() {
     for i in ~/workspace/*(/) ; do
         DIR=$(basename $i) ;
          alias $DIR="cd ~/workspace/$i";
     done
   }
   aliasgen
   #+end_src

   - a make a shortcut to the current directory.
   #+begin_src bash
     function a() { alias $1=cd\ $PWD; }

     mkdir -p /tmp/foo
     cd /tmp/my-very-long-thing
     a vlt
     cd /
     vlt
     echo $PWD   # /tmp/my-very-long-thing
   #+end_src

   - unhist. functions can create aliases, and functions can receive
     functions as parameters (as a string (function name)), so we can
     combine them to advise existing functions.
     #+begin_src bash
   unhist () {
     alias $1=" $1"
   }
   unhist unhist
   unhist grep
   unhist rg

   noglobber() {
       alias $1="noglob $1"
   }
   noglobber http
   noglobber curl
   noglobber git

     #+end_src

     #+RESULTS:

   - Problem: These commands do not compose. Combination of 2 of those
     doesn't work, because the second acts just on the textual
     representation that it received, not the current value of the
     alias.

   # Solution :
   #   #+begin_src bash
   #   alias-to() {
   #     alias $1 | sed -e "s/.*='//" -e "s/'\$//"
   #   }

   #   aliasappend() {
   #     local cmd
   #     if alias $1 >/dev/null; then
   #       cmd=$(alias-to $1)
   #     else
   #       cmd=$1
   #     fi
   #     echo $cmd
   #   }

   #   muter() {
   #     local c
   #     c=$(aliasappend $1)
   #     alias $1="$c >/dev/null"
   #   }

   #   unhist() {
   #     local c
   #     c=$(aliasappend $1)
   #     alias $1=" $c"
   #   }

   #   unhist ls
   #   muter ls
   #   ls
   # #+end_src
** Override (advise?) common functions
   Overriding commands is generally a bad practice as it violates the
   principle of least surprise, but there might be occasions (mostly
   in your local machine) where you can integrate awesome finetunnings
   to your toolbelt.

   Here we're going to get the original docker binary file
   location. After that we declare a function called =docker= that
   will proxy the parameters to the original =docker= program UNLESS
   you're calling =docker run=. In that case, we're injecting a mouted
   volume that mounts =/root/.bash_history= of the container to a file
   hosted in the host (duh). That's a pretty cool way of keeping a
   history of your recent commands in your containers, no matter how
   many times you start and kill them.

   #+begin_src bash
   DOCKER_ORIG=$(which docker)
   docker () {
       if [[ $1 == "run" ]]; then
           shift
           $DOCKER_ORIG run -v $HOME/.gojira-kongs/.gojira-home/.bash_history:/root/.bash_history "$@"
       else
           $DOCKER_ORIG "$@"
       fi
   }
   #+end_src

   I'm particularly fond of this trick, as it saved me tons of
   typing. But at a personal level, it was mindblowing that sharing
   this around the internet caused the most disparity of opinions.
   Also, I recently read the greate book "Docker in Practice" by [[https://github.com/ianmiell][Ian
   Miell]] and there's a snippet that is 99.9% like the one I
   created myself. That was a very cool moment.

* zsh                                                              :noexport:
** global aliases make a forth-y shell
** globbing
   **/*(/)
** examples of global aliases
   - lspdf -tr TL P XA evince
   - docker exec -u root -ti $(docker ps -q H1) bash
** pass commands around
   This one uses [[*DRY_RUN][DRY_RUN]]. While refactoring a script that does some
   curls, we want to make sure that our refactored version does the
   exact same calls in the same order.

   #+begin_src bash
   compare_outputs() {
     export DRY_RUN=1
     git checkout b1
     $@ 2>/tmp/1.out
     git checkout b2
     $@ 2>/tmp/2.out
     echo "diffing"
     diff /tmp/1.out /tmp/2.out
   }
   compare_outputs ./release.sh -p rhel:6 -R 'internal-preview'
   #+end_src

   First we create a function =compare_outputs=, that gets a command
   to run as parameters. The function will run it once, redirecting
   the standard error to a file =/tmp/1.out=.

   Then, it checks out the branch that contains our refactored
   version, and will run the command again, redirecting standard error
   to =/tmp/2.out=, and will diff the two outputs.

   In case there's a difference between the two, =diff= will output
   them, and the function will return the non-zero exit value of
   diff. If everything went fine, =compare_outputs= will succeed.

   Now that we know that for this inputs, the command runs fine, we
   want to find out if it works for other types of releases, not only
   internal-preview.

   Here I'm using zsh's global aliases to give a much more fluid
   interface to the commands:

   #+begin_src shell
   alias -g SPLIT='| tr " " "\n" '
   alias -g FORI='| while read i ; do '
   alias -g IROF='; done '

   set -e
   echo "ga internal-preview rc1 rc2" SPLIT FORI
      noglob compare_outputs ./release.sh -p rhel:8 -R "$i"
   IROF
   #+end_src

   So, combining the two, we can have a really smooth way of iterating
   over the possibilities, without really messing into the details of
   loops.

   WARNING: This approach is not robust enough to put it anywhere in
   production, but to write quick one off scripts is a
   killer. Experimenting in a shell and creating tools and 2nd order
   tools to make interaction faster builds a language that grows on
   you, and keeps improving your toolbelt.


** suffix aliases don't have to match a filename
   zsh has another type of aliases called "suffix alias". Those alias
   allow you to define programs to open/run file types.
   #+begin_src shell
   alias -s docx="libreoffice"
   #+end_src

   With this set, if you write a name of a file ending with =docx= as
   the first token in a command line, it will use libreoffice to open
   it.

   #+begin_src shell
   invoice1.docx
   # will effectively call libreoffice invoice1.docx
   #+end_src

   The trick here is that the parser doesn't check that the file is
   indeed an existing file. It can be any string.

   Let's look at an example of it.

   #+begin_src shell
   alias -s git="git clone"
   #+end_src

   In this case, we can easily copy a =git@github.com:.....git= from a
   browser, and paste it into a zsh console. Then, zsh will run that
   "file" with the command =git clone=, effectively cloning that
   repository.

   Cool, ain't it?

** noglob
   zsh has more aggressive parameter expansion, to the level that
   =[,],...= have special meanings, and will be interpreted and
   expanded before calling the final commands in your shell.

   There are commands that you don't want ever expanded , for example,
   when using =curl=, it's much more likely that an open bracket will
   be ment to be there verbatim rather than expanded.

   Zsh provides a command to quote the following expansions. And it's
   called noglob.
   #+begin_src bash
   noglob curl http://example.com\&a[]=1
   #+end_src

** make noglob 'transparent' to bash
   zsh and bash are mostly compatible, but there's a few things not
   supported in bash. =noglob= is one of them. To do to layer
   inbetween, an easy way is to just create a =~/bin/noglob= file
   #+begin_src bash
   $*
   #+end_src

* debugging                                                        :noexport:
** adding =bash= to a script to debug
** DRY_RUN
   #+begin_src bash
   if [[-n $DRY_RUN ]]; then
     curl () {
       echo curl $@
     }
   fi
   #+end_src
   use =command curl= to force the command, not the alias or anything

* patterns                                                         :noexport:
** undestand <(foo) and >(foo)
** use xargs
** use GNU Parallel
** append options in an array during the logic of the program
   And splat them in the cli of the command you're throwing (docker run?)
** trap
   - trap "rm $variable"
** debug by introducing a 'bash'
   insert =bash=
** debug with set -xa
   optargs "V" option; do
   case $option in
   V)
     set -xa
     ;;
** structure the app like a normal app
   Shellscripts are thought as quick one-off programs, but the truth
   is that they tend to be useful and sticky, so you better write them
   from the start as if it would be permanent.
*** Fail Fast
    - https://dougrichardson.us/2018/08/03/fail-fast-bash-scripting.html
*** Template
    Bash is extremely permissive in what it allows to be coded and
    ran. By default, failures do not make the program exit or throw an
    exeption (no exceptions here).

    A way to improve the defaults, is setting a bunch of flags that
    make the script stricter, so it fails on many red flag situations
    you'd want to stop anyway because something went wrong.
    #+begin_src bash
    #!/usr/bin/env bash
    set -eEuo pipefail
    shopt -s inherit_errexit

    main() {
      parse_args()
      validate_args()
      do_things()
      cleanup()
    }

    main "$@"
    #+end_src
** make steps of the process composable
   #+begin_src bash
   while [[ $# -gt 0 ]]; do
    key="$1"
    $key  # eval $key as a function
    shift
   done
   #+end_src
** use $@ when you can
   #+begin_src bash

   compare_outputs() {
     export DRY_RUN=1
     $@ 2>/tmp/1.out
     git checkout -
     $@ 2>/tmp/2.out
     git checkout -
     echo "diffing"
     diff /tmp/1.out /tmp/2.out
   }

   compare_outputs ./release.sh -u 1 -k 1 -p rhel:6 -v 1 -e -R 'internal-preview'

   # extra tricky
   alias -g SPLIT='| tr " " "\n" '
   alias -g FORI='| while read i ; do '
   alias -g IROF='; done '

   echo "ga internal-preview rc1 rc2" SPLIT FORI
     compare_outputs ./release.sh -u 1 -k 1 -p rhel:8 -v 1 -e -R "$i"
   IROF
   #+end_src
   - https://www.oilshell.org/blog/2017/01/13.html
** source files
   =source= is like =require= in most programming languages. It
   evaluates the sourced file in the context of the currently
   evaluated script.

   It's simple, but get used to modularize your code into libraries.

   Be careful, it's very rudimentary, and it will be overwriting old
   vars or functions if names clash. There's no namespacing happening
   there.

   #+begin_src bash

   source file.sh

   # the same
   . file.sh
   #+end_src
** Dots and colons allowed in function names!
   A way to split the namespace is to have libs define functions with
   their own namespace.

   I've gotten used to use dots or colons as namespace separator.
   #+begin_src bash
   semver.greater() {
    # ...
   }
   #+end_src
   or
   #+begin_src bash
   semver:greater() {
    # ...
   }
   #+end_src
** Use Scripts as a Libs
   A python-inspired way of using scripts as loadable libraries is to
   check whether the current file was the one that was called
   originally or it's being just sourced.

   Again, no side effects in load time makes this functionality
   possible. otherwise, you're on your own.
   #+begin_src bash
    # Allow sourcing of this script
    if [[ $(basename "$(realpath "$0")") == "package.sh" ]]; then
      setup
      parse_args "$@"
      main
    fi
   #+end_src
** Use tempfiles everywhere
   Your script is not going to run alone. Don't assume paths are
   fixed or known.

   CI/CD Pipelines run many jobs in the same node and files can start
   clashing.

   Make use of =$(mktemp /tmp/foo-bar.XXXXX)=.

   # #+begin_src bash
   # git_clone_tmp() {
   #   local repo=${1:?repo is required}
   #   local ref=${2:?ref is required}
   #   tmpath=$(mktemp -d "/tmp/kong-$repo-XXXXX")
   #   on_exit "rm -rf $tmpath"
   #   git clone -b ${ref} https://"$GITHUB_TOKEN"@github.com/my-org/${repo}.git $tmpath
   # }
   # #+end_src
** Use tempdirs everywhere

   Your script is not going to run alone. Don't assume paths are
   fixed or known.

   CI/CD Pipelines run many jobs in the same node and files can start
   clashing.

   Make use of =$(mktemp -d /tmp/foo-bar.XXXXX)=.  If you have to patch a
   file, do it in a clean fresh copy. Don't modify files in old paths

   If you HAVE TO modify paths, do it idempotently.  But really, don't do it. aa

   CAVEAT: You have to manually delete the directory if you want it cleaned.
** Cleanup tasks with trap
   =trap= is used to 'subscribe' a callback when something happens.
   Many times it's used on exit. It's a good thing to cleanup tmpdirs after your script
   exits, so you can use the output of =mktemp -d= and subscribe a cleanup
   function for it.

   #+begin_src bash
   on_exit() {
     rm -rf $1
   }

   trap "on_exit $(mktemp -d /tmp/foo-bar.XXXXX)" EXIT SIGINT
   #+end_src

** array of callbacks on_exit
   Level upping that pattern, we can have a helper to add callbacks to run on exit.

   #+begin_src bash

   ON_EXIT=()
   EXIT_RES=

   function on_exit_fn {
     EXIT_RES=$?
     for cb in "${ON_EXIT[@]}"; do $cb || true; done
     return $EXIT_RES
   }

   trap on_exit_fn EXIT SIGINT

   function on_exit {
     ON_EXIT+=("$@")
   }

   #+end_src
** pass flags as a splatted array
** explore what passes through a pipe with
   - https://stackoverflow.com/questions/17983777/shell-pipe-to-multiple-commands-in-a-file
   tee >(some_command) |
** inherit_errcode
** redirects
   - https://catonmat.net/ftp/bash-redirections-cheat-sheet.pdf
   - https://catonmat.net/bash-one-liners-explained-part-three
** pushd/popd
   pushd and popd are used to move to some directory and go back to it
   in a stack fashion, so nesting can happen and you never lose track.
   #+begin_src bash
   pushd /tmp/my-dir
     echo $PWD
   popd
   #+end_src

   Here's an alternative way, that at least makes sure that you close
   all pushd with a popd.

   Starting a new shell and cd-ing , will make all commands in that
   subshell be in that directory, and will come back to the old
   directory after closing the new spawned shell.

   #+begin_src bash
   (cd /tmp/my-dir
     ls
   )
   #+end_src
** wrap functions
   Bash can't pass blocks of code around, but the alternative is to
   pass functions.

   #+begin_src bash
   mute() {
     $@ >/dev/null
   }

   mute ls
   #+end_src
* links                                                            :noexport:
  - https://tldp.org/LDP/abs/html/
  - Gary Bernhardt. The Unix Chainsaw
  - https://news.ycombinator.com/item?id=23765123
  - https://medium.com/@joydeepubuntu/functional-programming-in-bash-145b6db336b7
  - https://www.youtube.com/watch?v=yD2ekOEP9sU
  - http://catern.com/posts/pipes.html
  - https://ebzzry.io/en/zsh-tips-1/
  - https://github.com/ssledz/bash-fun
* from shell to lisp and everything in between                     :noexport:
  - https://github.com/oilshell/oil
  - https://www.eigenbahn.com/2020/07/08/painless-emacs-remote-shells
  - https://news.ycombinator.com/item?id=24249646 rust
  - spencertipping @github
  - rash (racket shell)
  - zsh
  - https://github.com/liljencrantz/crush
  - https://github.com/artyom-poptsov/metabash
  - perl/python/ruby and migrate from bash to X using backticks
  - https://www.nushell.sh/
  - bocker
* make scripts compatible between them                             :noexport:
** create scripts that replace shell commands and do the same.
   to be able to run a zsh script with noglobs in bash, here's
   =~/bin/noglob=.
   #+begin_src bash
   #!/usr/bin/env bash
   $*
   #+end_src
